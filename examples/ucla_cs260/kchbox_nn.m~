% Neural network (with softmax classifier at the end) on a test data
% Input: test_d(mxf), train_d(mxf), train_label(1xm), where f is the number
% of features, m is the number of training data.
% Output: prediction of the test data.


function prediction = kchbox_nn(test_d,train_d,train_label,numClasses,hiddenLayersSize,...
    sparsityParam,lambda,beta)
addpath neural_network
%% Step 1 Load data

clc;
%read diastolic data, nxm, where m is the number of data point(feature).
%And n is the number of patients
raw_d = xlsread('combine_dia');   
dia = raw_d(:,2:end);

%read systolic data
raw_s = xlsread('combine_sys');   
systo = raw_s(:,2:end);

%read label
label = xlsread('label');
label = label(:,2);

% start label from 1, not 0
label = label + 1;

%% Step 2. Feature generation
fv = generate_features_vector(dia,systo);   % fv = feature vectors

inputSize = size(fv,2); %input size for neural network, i.e. 52
%% Step 3. NN parameters initialization
% weights include hidden layers, the softmax classifer weight, and the
% biases. The network structure is 52x26x26x2
clc;
[theta,netconfig] = initializeParameters(inputSize,hiddenLayersSize,numClasses);

%% Step 4. Train the neural network with a softmax(assume no bias in softmax)

%% Step 4.1 Check the cost function and see if it is correct
clc;
addpath neural_network/sparse_autoencoder/

[cost,grad] = NNCost(theta,inputSize,hiddenLayersSize,numClasses,...
     netconfig,lambda,fv,label);

DEBUG = false
if DEBUG
    checkNNCost;
end

%% Step 4.2 optimize the theta using minfunc (which is a gradient descent algorithm)
clc;
addpath neural_network/minFunc/

%  Use minFunc to minimize the function
options.HessUpate = 'lbfgs'; % Here, we use L-BFGS to optimize our cost
                          % function. Generally, for minFunc to work, you
                          % need a function pointer with two outputs: the
                          % function value and the gradient. In our problem,
                          % sparseAutoencoderCost.m satisfies this.
options.MaxIter = 100;	  % Maximum number of iterations of L-BFGS to run 
options.Display = 'iter';
options.GradObj = 'on';

[optTheta, cost] = minFunc( @(p) NNCost(p, ...
                                   inputSize, hiddenLayersSize, ...
                                   numClasses, netconfig, ...
                                   lambda, fv, label), ...
                              theta, options);

%% Step 5: Test: using cross validation
clc;

[pred] = NNPredict(optTheta, inputSize, hiddenLayersSize, ...
                          numClasses, netconfig, fv');                       

% evaluation
acc = mean(label(:) == pred(:));
fprintf('Accuracy: %0.3f%%\n', acc * 100);



end






